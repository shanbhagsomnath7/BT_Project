# -*- coding: utf-8 -*-
"""Brain_Tumor_Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cGr9dvuInMhiEyKGtR-iPEpY2d0hfUIX
"""


drive.mount('/content/drive')

# Step 1: Import necessary libraries
import os
import random
import matplotlib.pyplot as plt
from PIL import Image

# Step 2: Define the path to your dataset in Google Drive
# Make sure the folder name "BrainTumorProject" matches what you uploaded.
base_path = '/content/drive/MyDrive/BrainTumorProject/data/'
yes_path = os.path.join(base_path, 'yes')
no_path = os.path.join(base_path, 'no')

# Step 3: Get a list of all image files
try:
    yes_files = [os.path.join(yes_path, f) for f in os.listdir(yes_path)]
    no_files = [os.path.join(no_path, f) for f in os.listdir(no_path)]
    all_files = yes_files + no_files
    print(f"Found {len(yes_files)} 'yes' images and {len(no_files)} 'no' images.")

    # Step 4: Pick a random image and display it
    random_image_path = random.choice(all_files)

    # Determine the label based on the folder it came from
    label = "Tumor" if random_image_path in yes_files else "No Tumor"

    # Open and display the image
    img = Image.open(random_image_path)

    plt.imshow(img)
    plt.title(f'This is a sample image with the label: {label}')
    plt.axis('off') # Hides the x and y axis numbers
    plt.show()

except FileNotFoundError:
    print("Error: The specified directory was not found.")
    print("Please make sure the path is correct and your 'BrainTumorProject' folder is in your main Google Drive.")

# Import libraries we'll need
import cv2
import numpy as np
import os
from sklearn.model_selection import train_test_split

# --- Step 1: Load and Resize the Images ---

# Define the image size for our model
IMG_SIZE = 224

# Create empty lists to store our data and labels
X = [] # This will store the image arrays
y = [] # This will store the labels (0 for 'no', 1 for 'yes')

# Define the path to your data again
base_path = '/content/drive/MyDrive/BrainTumorProject/data/'
yes_path = os.path.join(base_path, 'yes')
no_path = os.path.join(base_path, 'no')

# Process the 'yes' images (with tumors)
for img_file in os.listdir(yes_path):
    img_path = os.path.join(yes_path, img_file)
    try:
        img_array = cv2.imread(img_path)
        resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
        X.append(resized_array)
        y.append(1) # 1 is the label for 'yes' (tumor)
    except Exception as e:
        print(f"Error processing file {img_path}: {e}")

# Process the 'no' images (no tumors)
for img_file in os.listdir(no_path):
    img_path = os.path.join(no_path, img_file)
    try:
        img_array = cv2.imread(img_path)
        resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
        X.append(resized_array)
        y.append(0) # 0 is the label for 'no' (no tumor)
    except Exception as e:
        print(f"Error processing file {img_path}: {e}")

print("Successfully loaded and resized all images.")
print(f"Total images: {len(X)}, Total labels: {len(y)}")

# --- Step 2: Convert to NumPy arrays and Normalize ---

# Convert the Python lists to NumPy arrays for efficient processing
X = np.array(X)
y = np.array(y)

# Normalize the image data. Dividing by 255 scales pixel values to be between 0 and 1.
X = X / 255.0

print("Converted lists to NumPy arrays and normalized pixel values.")
print(f"Shape of image data (X): {X.shape}")
print(f"Shape of label data (y): {y.shape}")

# --- Step 3: Split the Data into Training and Testing Sets ---

# We'll use 80% of the data for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("\nSplit data into training and testing sets.")
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_test: {y_test.shape}")

# Import the necessary libraries and layers from TensorFlow/Keras
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# --- Step 1: Build the Model using Transfer Learning ---

# Load the VGG16 model, pre-trained on the ImageNet dataset.
# We don't include the top (final) layer because we'll add our own.
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))

# Freeze the layers of the base model so they don't get re-trained.
# We want to leverage their pre-learned knowledge.
for layer in base_model.layers:
    layer.trainable = False

# Add our custom layers on top of the base model
# This is our custom "head" for the classifier
head_model = base_model.output
head_model = Flatten(name='flatten')(head_model)
head_model = Dense(512, activation='relu')(head_model)
head_model = Dropout(0.5)(head_model)
head_model = Dense(1, activation='sigmoid')(head_model) # Sigmoid for binary classification

# Combine the base model and our custom head to create the final model
model = Model(inputs=base_model.input, outputs=head_model)

print("Model architecture built successfully.")

# --- Step 2: Compile the Model ---

# We configure the model for training.
# Adam is an efficient optimizer, and binary_crossentropy is the standard loss function for two-class problems.
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary() # Prints a summary of the model's layers

# --- Step 3: Train the Model ---

print("\nStarting model training...")

# This is where the magic happens. The model learns from the training data.
# We'll train for 10 epochs (10 passes over the entire dataset).
history = model.fit(X_train, y_train,
                    epochs=10,
                    validation_data=(X_test, y_test),
                    batch_size=32)

print("\nModel training completed!")

# --- Step 4: Evaluate the Model ---

loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nFinal accuracy on the test set: {accuracy*100:.2f}%")

# --- Step 5: Save the Model ---

model.save('/content/drive/MyDrive/BrainTumorProject/brain_tumor_model.h5')
print("\nModel saved successfully to your Google Drive!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from PIL import Image
# import numpy as np
# import cv2
# from tensorflow.keras.models import load_model
# import sqlite3
# import pandas as pd
# from datetime import datetime
# 
# # --- Page Configuration ---
# st.set_page_config(page_title="Brain Tumor AI", layout="wide")
# 
# # --- Database Functions ---
# def setup_database():
#     conn = sqlite3.connect('predictions.db')
#     c = conn.cursor()
#     # NEW: Added a 'notes' column to the table
#     c.execute('''
#         CREATE TABLE IF NOT EXISTS predictions (
#             id INTEGER PRIMARY KEY,
#             timestamp DATETIME,
#             result TEXT,
#             confidence REAL,
#             notes TEXT
#         )
#     ''')
#     conn.commit()
#     conn.close()
# 
# # NEW: Updated function to log notes as well
# def log_prediction(result, confidence, notes):
#     conn = sqlite3.connect('predictions.db')
#     c = conn.cursor()
#     c.execute("INSERT INTO predictions (timestamp, result, confidence, notes) VALUES (?, ?, ?, ?)",
#               (datetime.now(), result, confidence, notes))
#     conn.commit()
#     conn.close()
# 
# setup_database()
# 
# # --- Preprocessing & Model Loading ---
# def preprocess_image(image):
#     img_array = np.array(image)
#     resized_array = cv2.resize(img_array, (224, 224))
#     normalized_array = resized_array / 255.0
#     expanded_array = np.expand_dims(normalized_array, axis=0)
#     return expanded_array
# 
# @st.cache_resource
# def load_ai_model():
#     model_path = '/content/drive/MyDrive/BrainTumorProject/brain_tumor_model.h5'
#     model = load_model(model_path)
#     return model
# model = load_ai_model()
# 
# # --- Navigation ---
# st.sidebar.title("Navigation")
# page = st.sidebar.radio("Go to", ["Doctor's Portal", "Admin Dashboard"])
# 
# # --- Doctor's Portal Page ---
# if page == "Doctor's Portal":
#     st.title("Doctor's Portal: Brain Tumor Detection AI ðŸ§ ")
#     uploaded_file = st.file_uploader("Choose an MRI image...", type=["jpg", "jpeg", "png"])
# 
#     if uploaded_file is not None:
#         image = Image.open(uploaded_file)
#         col1, col2 = st.columns(2)
#         with col1:
#             st.image(image, caption='Uploaded MRI.')
#         with col2:
#             if st.button('Predict'):
#                 with st.spinner('The AI is thinking...'):
#                     processed_image = preprocess_image(image)
#                     prediction = model.predict(processed_image)
#                     confidence = float(prediction[0][0])
# 
#                     if confidence > 0.5:
#                         result_text = "Tumor Detected"
#                         st.error(f"**Result:** {result_text} (Confidence: {confidence*100:.2f}%)")
#                     else:
#                         result_text = "No Tumor Detected"
#                         st.success(f"**Result:** {result_text} (Confidence: {(1-confidence)*100:.2f}%)")
# 
#                     # NEW: Add a text area for notes and a save button
#                     notes = st.text_area("Enter Doctor's Notes:")
#                     if st.button("Save Notes and Log Prediction"):
#                         final_confidence = confidence if confidence > 0.5 else 1 - confidence
#                         log_prediction(result_text, final_confidence, notes)
#                         st.success("Prediction and notes successfully logged!")
# 
# # --- Admin Dashboard Page ---
# elif page == "Admin Dashboard":
#     st.title("Admin Dashboard ðŸ“Š")
#     password = st.text_input("Enter password", type="password")
#     if password == "admin123":
#         st.success("Access Granted")
#         try:
#             conn = sqlite3.connect('predictions.db')
#             df = pd.read_sql_query("SELECT * FROM predictions", conn)
#             conn.close()
#             st.header("Prediction History with Notes")
#             st.dataframe(df.sort_values(by='timestamp', ascending=False))
#         except Exception as e:
#             st.error(f"Database error: {e}")
#     elif password:
#         st.error("Incorrect password.")



# Commented out IPython magic to ensure Python compatibility.
# # --- Create the 'pages' directory first ---
# !mkdir pages
# 
# # --- Now, write the Admin Dashboard file inside it ---
# %%writefile pages/Admin_Dashboard.py
# import streamlit as st
# import sqlite3
# import pandas as pd
# 
# st.set_page_config(page_title="Admin Dashboard", layout="wide")
# 
# st.title("Admin Dashboard ðŸ“Š")
# st.write("This page provides analytics on the AI model's usage and results.")
# 
# # --- Password Protection ---
# password = st.text_input("Enter password to access admin features", type="password")
# 
# if password == "admin123":
#     st.success("Access Granted")
# 
#     # --- Database Connection ---
#     try:
#         conn = sqlite3.connect('predictions.db')
#         # Load the data into a pandas DataFrame
#         df = pd.read_sql_query("SELECT * FROM predictions", conn)
#         conn.close()
# 
#         # --- Display Analytics ---
#         st.header("Key Metrics")
#         total_scans = len(df)
#         positive_detections = len(df[df['result'] == 'Tumor Detected'])
# 
#         col1, col2 = st.columns(2)
#         col1.metric("Total Scans Analyzed", total_scans)
#         if total_scans > 0:
#             col2.metric("Positive Detection Rate", f"{(positive_detections/total_scans)*100:.2f}%")
#         else:
#             col2.metric("Positive Detection Rate", "0.00%")
# 
#         st.header("Prediction History")
#         # Display the dataframe, showing the most recent predictions first
#         st.dataframe(df.sort_values(by='timestamp', ascending=False))
# 
#         st.header("Results Breakdown")
#         if not df.empty:
#             pie_chart_data = df['result'].value_counts()
#             st.bar_chart(pie_chart_data)
# 
#     except Exception as e:
#         st.error(f"An error occurred while accessing the database: {e}")
# 
# elif password and password != "":
#     st.error("Incorrect password.")

from pyngrok import ngrok

# --- Step 1: Stop any old ngrok tunnels ---
ngrok.kill()

# --- Step 2: Clear Streamlit's cache ---
# This deletes any cached versions of your old app


# --- Step 3: Relaunch the app ---
NGROK_AUTH_TOKEN = "32xM4cXu3Ra4NNeS0ZiDEJlrLTm_7gt27FWWw7wxLRCpBD6BH"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

public_url = ngrok.connect(8501)
print(' * Tunnel URL:', public_url)

